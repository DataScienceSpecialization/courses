---
title       : Inference basics
subtitle    : 
author      : Jeffrey Leek, Assistant Professor of Biostatistics 
job         : Johns Hopkins Bloomberg School of Public Health
logo        : bloomberg_shield.png
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow   # 
url:
  lib: ../../libraries
  assets: ../../assets
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
---


```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F}
# make this an external chunk that can be included in any file
options(width = 100)
par(mar=c(5,4,1,2))
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Fit a line to the Galton Data

```{r galton,fig.height=4,fig.width=4}
library(UsingR); data(galton);
plot(galton$parent,galton$child,pch=19,col="blue")
lm1 <- lm(galton$child ~ galton$parent)
lines(galton$parent,lm1$fitted,col="red",lwd=3)
```

---

## Fit a line to the Galton Data

```{r, dependson="galton",fig.height=4,fig.width=4}
lm1
```


---

## Create a "population" of 1 million families

```{r newGalton,dependson="galton",fig.height=4,fig.width=4}
newGalton <- data.frame(parent=rep(NA,1e6),child=rep(NA,1e6))
newGalton$parent <- rnorm(1e6,mean=mean(galton$parent),sd=sd(galton$parent))
newGalton$child <- lm1$coeff[1] + lm1$coeff[2]*newGalton$parent + rnorm(1e6,sd=sd(lm1$residuals))
smoothScatter(newGalton$parent,newGalton$child)
abline(lm1,col="red",lwd=3)
```


---

## Let's take a sample

```{r sampleGalton1 ,dependson="newGalton",fig.height=4,fig.width=4}
set.seed(134325); sampleGalton1 <- newGalton[sample(1:1e6,size=50,replace=F),]
sampleLm1 <- lm(sampleGalton1$child ~ sampleGalton1$parent)
plot(sampleGalton1$parent,sampleGalton1$child,pch=19,col="blue")
lines(sampleGalton1$parent,sampleLm1$fitted,lwd=3,lty=2)
abline(lm1,col="red",lwd=3)
```

---

## Let's take another sample

```{r sampleGalton2 ,dependson="sampleGalton1",fig.height=4,fig.width=4}
sampleGalton2 <- newGalton[sample(1:1e6,size=50,replace=F),]
sampleLm2 <- lm(sampleGalton2$child ~ sampleGalton2$parent)
plot(sampleGalton2$parent,sampleGalton2$child,pch=19,col="blue")
lines(sampleGalton2$parent,sampleLm2$fitted,lwd=3,lty=2)
abline(lm1,col="red",lwd=3)
```

---

## Let's take another sample

```{r sampleGalton3 ,dependson="sampleGalton2",fig.height=4,fig.width=4}
sampleGalton3 <- newGalton[sample(1:1e6,size=50,replace=F),]
sampleLm3 <- lm(sampleGalton3$child ~ sampleGalton3$parent)
plot(sampleGalton3$parent,sampleGalton3$child,pch=19,col="blue")
lines(sampleGalton3$parent,sampleLm3$fitted,lwd=3,lty=2)
abline(lm1,col="red",lwd=3)
```

---

## Many samples

```{r manySamples,dependson="sampleGalton2",fig.height=4,fig.width=4}
sampleLm <- vector(100,mode="list")
for(i in 1:100){
  sampleGalton <- newGalton[sample(1:1e6,size=50,replace=F),]
  sampleLm[[i]] <- lm(sampleGalton$child ~ sampleGalton$parent)
}
```


---

## Many samples

```{r ,dependson="manySamples",fig.height=4,fig.width=4}
smoothScatter(newGalton$parent,newGalton$child)
for(i in 1:100){abline(sampleLm[[i]],lwd=3,lty=2)}
abline(lm1,col="red",lwd=3)
```


---

## Histogram of estimates


```{r ,dependson="manySamples",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
hist(sapply(sampleLm,function(x){coef(x)[1]}),col="blue",xlab="Intercept",main="")
hist(sapply(sampleLm,function(x){coef(x)[2]}),col="blue",xlab="Slope",main="")
```

---

## Distribution of coefficients

From the [central limit theorem](https://www.khanacademy.org/math/probability/statistics-inferential/sampling_distribution/v/central-limit-theorem) it turns out that in many cases:

$$\hat{b}_0 \sim N(b_0, Var(\hat{b}_0))$$
$$\hat{b}_1 \sim N(b_0, Var(\hat{b}_1))$$

which we can estimate with:

$$\hat{b}_0 \approx N(b_0, \hat{Var}(\hat{b}_0))$$
$$\hat{b}_1 \approx N(b_0, \hat{Var}(\hat{b}_1))$$

$\sqrt{\hat{Var}(\hat{b}_0)}$ is the "standard error" of the estimate $\hat{b}_0$ and is abbreviated $S.E.(\hat{b}_0)$


---

## Estimating the values in R

```{r sampleReg,dependson="newGalton"}
sampleGalton4 <- newGalton[sample(1:1e6,size=50,replace=F),]
sampleLm4 <- lm(sampleGalton4$child ~ sampleGalton4$parent)
summary(sampleLm4)
```

---

## Estimating the values in R

```{r,dependson="sampleReg",fig.height=4,fig.width=4}
hist(sapply(sampleLm,function(x){coef(x)[2]}),col="blue",xlab="Slope",main="",freq=F)
lines(seq(0,5,length=100),dnorm(seq(0,5,length=100),mean=coef(sampleLm4)[2],
      sd=summary(sampleLm4)$coeff[2,2]),lwd=3,col="red")
```


---

## Why do we standardize?

<img class=center src=../../assets/img/therm.jpg height=350>

$$K^{\circ} = C^{\circ} + 273.15 $$
$$K^{\circ} = \frac{F^{\circ} + 459.67}{1.8}$$
 
[http://en.wikipedia.org/wiki/Kelvin](http://en.wikipedia.org/wiki/Kelvin)


---

## Why do we standardize?


```{r ,dependson="manySamples",fig.height=4,fig.width=8}
par(mfrow=c(1,2))
hist(sapply(sampleLm,function(x){coef(x)[1]}),col="blue",xlab="Intercept",main="")
hist(sapply(sampleLm,function(x){coef(x)[2]}),col="blue",xlab="Slope",main="")
```


---

## Standardized coefficients

$$\hat{b}_0 \approx N(b_0, \hat{Var}(\hat{b}_0))$$
$$\hat{b}_1 \approx N(b_0, \hat{Var}(\hat{b}_1))$$

and

$$\frac{\hat{b}_0 - b_0}{S.E.(\hat{b}_0)} \sim t_{n-2} $$
$$\frac{\hat{b}_1 - b_1}{S.E.(\hat{b}_1)} \sim t_{n-2}$$

Degrees of Freedom $\approx$ number of samples - number of things you estimated. 


---

## $t_{n-2}$ versus $N(0,1)$

```{r,fig.height=4,fig.width=4.5}
x <- seq(-5,5,length=100)
plot(x,dnorm(x),type="l",lwd=3)
lines(x,dt(x,df=3),lwd=3,col="red")
lines(x,dt(x,df=10),lwd=3,col="blue")
```

---

## Confidence intervals

We have an estimate $\hat{b}_1$ and we want to know something about how good our estimate is. 

One way is to create a "level $\alpha$ confidence interval".

A confidence interval will include the real parameter $\alpha$ percent of the time in repeated studies. 


---

## Confidence intervals

$$(\hat{b}_1 + T_{\alpha/2}\times S.E.(\hat{b}_1),\hat{b}_1 - T_{\alpha/2} \times S.E.(\hat{b}_1))$$

```{r,dependson="sampleReg"}
summary(sampleLm4)$coeff
confint(sampleLm4,level=0.95)
```


---

## Confidence intervals

```{r,dependson="manySamples",fig.height=3,fig.width=3}
par(mar=c(4,4,0,2));plot(1:10,type="n",xlim=c(0,1.5),ylim=c(0,100),
                         xlab="Coefficient Values",ylab="Replication")
for(i in 1:100){
    ci <- confint(sampleLm[[i]]); color="red";
    if((ci[2,1] < lm1$coeff[2]) & (lm1$coeff[2] < ci[2,2])){color = "grey"}
    segments(ci[2,1],i,ci[2,2],i,col=color,lwd=3)
}
lines(rep(lm1$coeff[2],100),seq(0,100,length=100),lwd=3)
```


---

## How you report the inference

```{r,dependson="sampleReg"}
sampleLm4$coeff
confint(sampleLm4,level=0.95)
```

A one inch increase in parental height is associated with a 0.77 inch increase in child's height (95% CI: 0.42-1.12 inches).
