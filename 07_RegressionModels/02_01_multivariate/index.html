<!DOCTYPE html>
<html>
<head>
  <title>Multivariable regression</title>
  <meta charset="utf-8">
  <meta name="description" content="Multivariable regression">
  <meta name="author" content="Brian Caffo, Roger Peng and Jeff Leek">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="../../librariesNew/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="../../librariesNew/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="../../librariesNew/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="../../librariesNew/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  
  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../librariesNew/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="../../librariesNew/frameworks/io2012/js/slides" 
    src="../../librariesNew/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <aside class="gdbar">
    <img src="../../assets/img/bloomberg_shield.png">
  </aside>
  <hgroup class="auto-fadein">
    <h1>Multivariable regression</h1>
    <h2></h2>
    <p>Brian Caffo, Roger Peng and Jeff Leek<br/>Johns Hopkins Bloomberg School of Public Health</p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <article data-timings="">
    <pre><code>## Error: object &#39;opts_chunk&#39; not found
</code></pre>

<pre><code>## Error: object &#39;knit_hooks&#39; not found
</code></pre>

<pre><code>## Error: object &#39;knit_hooks&#39; not found
</code></pre>

<h2>Multivariable regression analyses</h2>

<ul>
<li>If I were to present evidence of a relationship between 
breath mint useage (mints per day, X) and pulmonary function
(measured in FEV), you would be skeptical.

<ul>
<li>Likely, you would say, &#39;smokers tend to use more breath mints than non smokers, smoking is related to a loss in pulmonary function. That&#39;s probably the culprit.&#39;</li>
<li>If asked what would convince you, you would likely say, &#39;If non-smoking breath mint users had lower lung function than non-smoking non-breath mint users and, similarly, if smoking breath mint users had lower lung function than smoking non-breath mint users, I&#39;d be more inclined to believe you&#39;.</li>
</ul></li>
<li>In other words, to even consider my results, I would have to demonstrate that they hold while holding smoking status fixed.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>Multivariable regression analyses</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>An insurance company is interested in how last year&#39;s claims can predict a person&#39;s time in the hospital this year. 

<ul>
<li>They want to use an enormous amount of data contained in claims to predict a single number. Simple linear regression is not equipped to handle more than one predictor.</li>
</ul></li>
<li>How can one generalize SLR to incoporate lots of regressors for
the purpose of prediction?</li>
<li>What are the consequences of adding lots of regressors? 

<ul>
<li>Surely there must be consequences to throwing variables in that aren&#39;t related to Y?</li>
<li>Surely there must be consequences to omitting variables that are?</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>The linear model</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>The general linear model extends simple linear regression (SLR)
by adding terms linearly into the model.
\[
Y_i =  \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots +
\beta_{p} X_{pi} + \epsilon_{i} 
= \sum_{k=1}^p X_{ik} \beta_j + \epsilon_{i}
\]</li>
<li>Here \(X_{1i}=1\) typically, so that an intercept is included.</li>
<li>Least squares (and hence ML estimates under iid Gaussianity 
of the errors) minimizes
\[
\sum_{i=1}^n \left(Y_i - \sum_{k=1}^p X_{ki} \beta_j\right)^2
\]</li>
<li>Note, the important linearity is linearity in the coefficients.
Thus
\[
Y_i =  \beta_1 X_{1i}^2 + \beta_2 X_{2i}^2 + \ldots +
\beta_{p} X_{pi}^2 + \epsilon_{i} 
\]
is still a linear model. (We&#39;ve just squared the elements of the
predictor variables.)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>How to get estimates</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Recall that the LS estimate for regression through the origin, \(E[Y_i]=X_{1i}\beta_1\), was \(\sum X_i Y_i / \sum X_i^2\).</li>
<li>Let&#39;s consider two regressors, \(E[Y_i] = X_{1i}\beta_1 + X_{2i}\beta_2 = \mu_i\). </li>
<li>Least squares tries to minimize
\[
\sum_{i=1}^n (Y_i - X_{1i} \beta_1 - X_{2i} \beta_2)^2
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Result</h2>
  </hgroup>
  <article data-timings="">
    <p>\[\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}^2}\]</p>

<ul>
<li>That is, the regression estimate for \(\beta_1\) is the regression 
through the origin estimate having regressed \(X_2\) out of both
the response and the predictor.</li>
<li>(Similarly, the regression estimate for \(\beta_2\) is the regression  through the origin estimate having regressed \(X_1\) out of both the response and the predictor.)</li>
<li>More generally, multivariate regression estimates are exactly those having removed the linear relationship of the other variables
from both the regressor and response.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>Example with two variables, simple linear regression</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>\(Y_{i} = \beta_1 X_{1i} + \beta_2 X_{2i}\) where  \(X_{2i} = 1\) is an intercept term.</li>
<li>Notice the fitted coefficient of \(X_{2i}\) on \(Y_{i}\) is \(\bar Y\)

<ul>
<li>The residuals \(e_{i, Y | X_2} = Y_i - \bar Y\)</li>
</ul></li>
<li>Notice the fitted coefficient of \(X_{2i}\) on \(X_{1i}\) is \(\bar X_1\)

<ul>
<li>The residuals \(e_{i, X_1 | X_2}= X_{1i} - \bar X_1\)</li>
</ul></li>
<li>Thus
\[
\hat \beta_1 = \frac{\sum_{i=1}^n e_{i, Y | X_2} e_{i, X_1 | X_2}}{\sum_{i=1}^n e_{i, X_1 | X_2}^2} = \frac{\sum_{i=1}^n (X_i - \bar X)(Y_i - \bar Y)}{\sum_{i=1}^n (X_i - \bar X)^2}
= Cor(X, Y) \frac{Sd(Y)}{Sd(X)}
\]</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>The general case</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Least squares solutions have to minimize
\[
\sum_{i=1}^n (Y_i - X_{1i}\beta_1 - \ldots - X_{pi}\beta_p)^2
\]</li>
<li>The least squares estimate for the coefficient of a multivariate regression model is exactly regression through the origin with the linear relationships with the other regressors removed from both the regressor and outcome by taking residuals. </li>
<li>In this sense, multivariate regression &quot;adjusts&quot; a coefficient for the linear impact of the other variables. </li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>Demonstration that it works using an example</h2>
  </hgroup>
  <article data-timings="">
    <h3>Linear model with two variables</h3>

<pre><code class="r">n = 100; x = rnorm(n); x2 = rnorm(n); x3 = rnorm(n)
y = 1 + x + x2 + x3 + rnorm(n, sd = .1)
ey = resid(lm(y ~ x2 + x3))
ex = resid(lm(x ~ x2 + x3))
sum(ey * ex) / sum(ex ^ 2)
</code></pre>

<pre><code>## [1] 1.009
</code></pre>

<pre><code class="r">coef(lm(ey ~ ex - 1))
</code></pre>

<pre><code>##    ex 
## 1.009
</code></pre>

<pre><code class="r">coef(lm(y ~ x + x2 + x3)) 
</code></pre>

<pre><code>## (Intercept)           x          x2          x3 
##      1.0202      1.0090      0.9787      1.0064
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Interpretation of the coeficients</h2>
  </hgroup>
  <article data-timings="">
    <p>\[E[Y | X_1 = x_1, \ldots, X_p = x_p] = \sum_{k=1}^p x_{k} \beta_k\]</p>

<p>\[
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p] = (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k
\]</p>

<p>\[
E[Y | X_1 = x_1 + 1, \ldots, X_p = x_p]  - E[Y | X_1 = x_1, \ldots, X_p = x_p]\]
\[= (x_1 + 1) \beta_1 + \sum_{k=2}^p x_{k} \beta_k + \sum_{k=1}^p x_{k} \beta_k = \beta_1 \]
So that the interpretation of a multivariate regression coefficient is the expected change in the response per unit change in the regressor, holding all of the other regressors fixed.</p>

<p>In the next lecture, we&#39;ll do examples and go over context-specific
interpretations.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Fitted values, residuals and residual variation</h2>
  </hgroup>
  <article data-timings="">
    <p>All of our SLR quantities can be extended to linear models</p>

<ul>
<li>Model \(Y_i = \sum_{k=1}^p X_{ik} \beta_{k} + \epsilon_{i}\) where \(\epsilon_i \sim N(0, \sigma^2)\)</li>
<li>Fitted responses \(\hat Y_i = \sum_{k=1}^p X_{ik} \hat \beta_{k}\)</li>
<li>Residuals \(e_i = Y_i - \hat Y_i\)</li>
<li>Variance estimate \(\hat \sigma^2 = \frac{1}{n-p} \sum_{i=1}^n e_i ^2\)</li>
<li>To get predicted responses at new values, \(x_1, \ldots, x_p\), simply plug them into the linear model \(\sum_{k=1}^p x_{k} \hat \beta_{k}\)</li>
<li>Coefficients have standard errors, \(\hat \sigma_{\hat \beta_k}\), and
\(\frac{\hat \beta_k - \beta_k}{\hat \sigma_{\hat \beta_k}}\)
follows a \(T\) distribution with \(n-p\) degrees of freedom.</li>
<li>Predicted responses have standard errors and we can calculate predicted and expected response intervals.</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Linear models</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>Linear models are the single most important applied statistical and machine learning techniqe, <em>by far</em>.</li>
<li>Some amazing things that you can accomplish with linear models

<ul>
<li>Decompose a signal into its harmonics.</li>
<li>Flexibly fit complicated functions.</li>
<li>Fit factor variables as predictors.</li>
<li>Uncover complex multivariate relationships with the response.</li>
<li>Build accurate prediction models.</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title=''>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Multivariable regression analyses'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='The linear model'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='How to get estimates'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Result'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Example with two variables, simple linear regression'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='The general case'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Demonstration that it works using an example'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Interpretation of the coeficients'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Fitted values, residuals and residual variation'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Linear models'>
         11
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="../../librariesNew/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="../../librariesNew/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>